{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priscilla97/llm-rag-foundations/blob/main/02_fine_tuning/3_A_full_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRQpKOxEmG1I"
      },
      "source": [
        "# A full training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou6srV2lmG1T"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QKXLRjamG1W"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "!pip install accelerate\n",
        "# To run the training on TPU, you will need to uncomment the following line:\n",
        "# !pip install cloud-tpu-client==0.10 torch==1.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare model and dataset"
      ],
      "metadata": {
        "id": "63m8LIMwmcL2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UztD1r6ZmG1b"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare for training\n",
        "We will need to define a few objects.\n",
        "1) the **dataloaders** we will use to iterate over batches.\n",
        "\n",
        "But before, we need to apply a bit of **postprocessing** to our **tokenized_datasets**, to take care of some things that the Trainer did for us automatically.\n",
        "\n",
        "Specifically, we need to:\n",
        "\n",
        "- Remove the columns corresponding to values the model does not expect (like the sentence1 and sentence2 columns).\n",
        "\n",
        "- Rename the column label to labels (because the model expects the argument to be named labels).\n",
        "\n",
        "- Set the format of the datasets so they return PyTorch tensors instead of lists.\n",
        "\n",
        "Our tokenized_datasets has one method for each of those steps:"
      ],
      "metadata": {
        "id": "USo1MDlemji6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IS6TXx7mG1d"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "tokenized_datasets[\"train\"].column_names"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then check that the result only has columns that our model will accept:"
      ],
      "metadata": {
        "id": "rIV3qG1lnI7d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQ0foC-5mG1h"
      },
      "outputs": [],
      "source": [
        "[\"attention_mask\", \"input_ids\", \"labels\", \"token_type_ids\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that this is done, we can easily define our **dataloaders**:"
      ],
      "metadata": {
        "id": "97ecNUq9nKrC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsHChKofmG1j"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To quickly check there is no mistake in the data processing, we can inspect a **batch** like this:"
      ],
      "metadata": {
        "id": "URMoQ-a5nT4f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd7A-DfWmG1l",
        "outputId": "0c6e0be0-21e0-4f1a-cd86-fad3b47822f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'attention_mask': torch.Size([8, 65]),\n",
              " 'input_ids': torch.Size([8, 65]),\n",
              " 'labels': torch.Size([8]),\n",
              " 'token_type_ids': torch.Size([8, 65])}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for batch in train_dataloader:\n",
        "    break\n",
        "{k: v.shape for k, v in batch.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We instantiate the **model** exactly as we did in the previous section:"
      ],
      "metadata": {
        "id": "pk2GP3tgnjsw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1qUbcGUmG1p"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make sure that everything will go smoothly during training, we pass our batch to this model:"
      ],
      "metadata": {
        "id": "1vN-QwUAnsLD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gNRdIvDmG1s",
        "outputId": "677a14a2-440b-46b8-f3b8-e6397d573208"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(0.5441, grad_fn=<NllLossBackward>) torch.Size([8, 2])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**batch)\n",
        "print(outputs.loss, outputs.logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All ðŸ¤— Transformers models will return the **loss** when labels are provided, and we also get the **logits** (two for each input in our batch, so a tensor of size 8 x 2).\n",
        "\n",
        "Weâ€™re almost ready to write our training loop!\n",
        "\n",
        "- an **optimizer** (eg AdamW)\n",
        "- a learning rate scheduler.\n",
        "\n",
        "**AdamW**, which is the same as Adam, but with a twist for weight decay regularization.\n",
        "\n",
        "**Modern Optimization Tips**: For even better performance, you can try:\n",
        "\n",
        "- AdamW with weight decay: AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
        "\n",
        "- 8-bit Adam: Use bitsandbytes for memory-efficient optimization\n",
        "Different learning rates: Lower learning rates (1e-5 to 3e-5) often work better for large models\n",
        "\n",
        "\n",
        "ðŸš€ Optimization Resources: Learn more about optimizers and training strategies in the ðŸ¤— Transformers optimization guide: https://huggingface.co/docs/transformers/main/en/performance#optimizer"
      ],
      "metadata": {
        "id": "2c3XwlGbnrs7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jw80qc4mG1u"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **learning rate scheduler** used by default is just a linear decay from the maximum value (5e-5) to 0.\n",
        "\n",
        "To properly define it, we need to know:\n",
        "- the number of **training steps** we will take, (num_epochs we want to run)\n",
        "- multiplied by the number of **training batches** (the length of our training dataloader).\n",
        "\n",
        "The Trainer uses three epochs by default, so we will follow that:"
      ],
      "metadata": {
        "id": "ZrIf_1kwotd4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QrTXMkTmG1v",
        "outputId": "68aa8b93-9ab1-4ed0-f0f6-0c2d97e41552"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1377"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "print(num_training_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The training loop\n",
        "One last thing: we will want to use the **GPU** if we have access to one (on a CPU, training might take several hours instead of a couple of minutes).\n",
        "\n",
        "To do this, we **define a device** we will put our model and our batches on:"
      ],
      "metadata": {
        "id": "fR0NVguApR51"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klIWhOFFmG1x",
        "outputId": "ab86ab25-4dd2-4e5b-a280-c2200d24df5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to train! To get some sense of when training will be finished, we add a progress bar over our number of training steps, using the tqdm library:"
      ],
      "metadata": {
        "id": "78MB0ihtpaVq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ7T7virmG1y"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modern Training Optimizations: To make your training loop even more efficient, consider:\n",
        "\n",
        "- **Gradient Clipping**: Add *torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)* before optimizer.step()\n",
        "\n",
        "- **Mixed Precision**: Use *torch.cuda.amp.autocast()* and *GradScaler* for faster training\n",
        "\n",
        "- **Gradient Accumulation**: Accumulate gradients over multiple batches to simulate larger batch sizes\n",
        "\n",
        "- **Checkpointing**: Save model checkpoints periodically to resume training if interrupted\n",
        "\n",
        "ðŸ”§ Implementation Guide: For detailed examples of these optimizations, see the ðŸ¤— Transformers efficient training guide(https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one) and the range of optimizers (https://huggingface.co/docs/transformers/main/en/optimizers)."
      ],
      "metadata": {
        "id": "PU7JyvQQp00Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The evaluation loop\n",
        "Weâ€™ve already seen the metric.compute() method, but metrics can actually accumulate batches for us as we go over the prediction loop with the method add_batch().\n",
        "\n",
        "Once we have accumulated all the batches, we can get the final result with metric.compute().\n",
        "\n",
        "Hereâ€™s how to implement all of this in an evaluation loop:"
      ],
      "metadata": {
        "id": "IxF1V5UPqem4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akC8uYVhmG1z",
        "outputId": "ace5701e-a206-4ffb-88c8-a7b17226db27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'accuracy': 0.8431372549019608, 'f1': 0.8907849829351535}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "model.eval()\n",
        "for batch in eval_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "metric.compute()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, your results will be slightly different because of the randomness in the model head initialization and the data shuffling, but they should be in the same ballpark."
      ],
      "metadata": {
        "id": "gMRwLersqxGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Supercharge your training loop with ðŸ¤— Accelerate\n",
        "\n",
        "Using the ðŸ¤— Accelerate library, with just a few adjustments we can enable distributed training on multiple GPUs or TPUs.\n",
        "\n",
        "Starting from the creation of the training and validation dataloaders, here is what our manual training loop looks like:"
      ],
      "metadata": {
        "id": "ol_bySSgq7LC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHR4jLR9mG10"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, get_scheduler\n",
        "from torch.optim import AdamW\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) The **first** line to add is the import line.\n",
        "\n",
        "2) The **second** line instantiates an **Accelerator** object that will look at the environment and initialize the proper distributed setup.\n",
        "\n",
        "3) remove the lines that put the model on the device.\n",
        "\n",
        "4) line that sends the dataloaders, the model, and the optimizer to **accelerator.prepare().**\n",
        "\n",
        "5) The remaining changes to make are removing the line that puts the batch on the device (again, if you want to keep this you can just change it to use accelerator.device) and **replacing loss.backward() with accelerator.backward(loss).**"
      ],
      "metadata": {
        "id": "jZnPBEB4rJHQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aoi52-OmG11"
      },
      "outputs": [],
      "source": [
        "# 1) import\n",
        "from accelerate import Accelerator\n",
        "from transformers import AutoModelForSequenceClassification, get_scheduler\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# 2) accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
        "\n",
        "# 3) remove device and model.to(device)\n",
        "\n",
        "# 4) accelerato.prepare:\n",
        "train_dl, eval_dl, model, optimizer = accelerator.prepare(\n",
        "    train_dataloader, eval_dataloader, model, optimizer\n",
        ")\n",
        "\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dl)\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dl:\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        # 5) accelerato.backward\n",
        "        accelerator.backward(loss)\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting this in a train.py script will make that script runnable on any kind of distributed setup. To try it out in your distributed setup, run the command:\n",
        "\n",
        "  accelerate config\n",
        "\n",
        "which will prompt you to answer a few questions and dump your answers in a configuration file used by this command:\n",
        "\n",
        "  accelerate launch train.py\n",
        "\n",
        "which will launch the distributed training.\n",
        "\n",
        "If you want to try this in a Notebook (for instance, to test it with TPUs on Colab), just paste the code in a training_function() and run a last cell with:"
      ],
      "metadata": {
        "id": "atlEokpJt2jj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ums-dqnemG12"
      },
      "outputs": [],
      "source": [
        "from accelerate import notebook_launcher\n",
        "\n",
        "notebook_launcher(training_function)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "A full training",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}