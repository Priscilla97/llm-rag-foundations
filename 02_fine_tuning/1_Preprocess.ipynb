{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priscilla97/llm-rag-foundations/blob/main/02_fine_tuning/1_Preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrxaSxnqMGtM"
      },
      "source": [
        "# Processing the data (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9vhluQeMGtP"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2obdcQDMGtP"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3c9Li0oMGtQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Same as before\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequences = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"This course is amazing!\",\n",
        "]\n",
        "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# This is new\n",
        "batch[\"labels\"] = torch.tensor([1, 1])\n",
        "\n",
        "optimizer = AdamW(model.parameters())\n",
        "loss = model(**batch).loss\n",
        "loss.backward()\n",
        "optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading a dataset from the Hub\n",
        "Of course, just training the model on two sentences is not going to yield very good results. To get better results, you will need to prepare a bigger dataset.\n",
        "\n",
        "We will use as an example the MRPC (Microsoft Research Paraphrase Corpus) dataset, introduced in a paper by William B. Dolan and Chris Brockett.\n",
        "\n",
        "The dataset consists of 5,801 pairs of sentences, with a label indicating if they are paraphrases or not (i.e., if both sentences mean the same thing).\n",
        "\n",
        "Dataset Documentation: https://huggingface.co/docs/datasets/index\n",
        "\n",
        "The ðŸ¤— Datasets library provides a very simple command to download and cache a dataset on the Hub. We can download the MRPC dataset like this:"
      ],
      "metadata": {
        "id": "5Pf8ici1MQqt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNWwxzaaMGtR",
        "outputId": "57732eba-2b25-43df-8a08-babf92b8a12b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 3668\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 408\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
              "        num_rows: 1725\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, we get a **DatasetDict** object which contains the *training set, the validation set, and the test set*.\n",
        "\n",
        "Each of those contains several **columns** (sentence1, sentence2, label, and idx) and a variable **number of rows**, which are the number of elements in each set (so, there are 3,668 pairs of sentences in the training set, 408 in the validation set, and 1,725 in the test set).\n",
        "\n",
        "*This command downloads and caches the dataset, by default in ~/.cache/huggingface/datasets. Recall from Chapter 2 that you can customize your cache folder by setting the HF_HOME environment variable.*"
      ],
      "metadata": {
        "id": "6-tuespDNayT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can access each pair of sentences in our raw_datasets object by indexing, like with a dictionary:"
      ],
      "metadata": {
        "id": "mp-5jS-MN0JM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2JGqlGRMGtR",
        "outputId": "ba89b407-9c4f-4b4b-cd7f-b0898a857919"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'idx': 0,\n",
              " 'label': 1,\n",
              " 'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
              " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_train_dataset = raw_datasets[\"train\"]\n",
        "raw_train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the **labels are already integers**, so we wonâ€™t have to do any preprocessing there.\n",
        "\n",
        "To know which integer corresponds to which label, we can inspect the **features** of our raw_train_dataset. This will tell us the **type of each column:**"
      ],
      "metadata": {
        "id": "Vg06oLF7N7Fa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2HWxI-8MGtS",
        "outputId": "54c218ff-13bb-406b-ab7a-e31aa61d79c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'sentence1': Value(dtype='string', id=None),\n",
              " 'sentence2': Value(dtype='string', id=None),\n",
              " 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),\n",
              " 'idx': Value(dtype='int32', id=None)}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_train_dataset.features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Behind the scenes, label is of type ClassLabel, and the mapping of integers to label name is stored in the names folder. 0 corresponds to not_equivalent, and 1 corresponds to equivalent."
      ],
      "metadata": {
        "id": "0-ln_Th_OfV9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing a dataset\n",
        "To preprocess the dataset, we need to **convert the text to numbers** the model can make sense of. This is done with a **tokenizer**.\n",
        "\n",
        "Tokenizer guide: https://huggingface.co/docs/transformers/main/en/tokenizer_summary\n",
        "\n",
        "We can feed the tokenizer one sentence or a list of sentences, so we can directly tokenize all the first sentences and all the second sentences of each pair like this:"
      ],
      "metadata": {
        "id": "Oa-P3echOkkq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53wEz8GnMGtS"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
        "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we canâ€™t just pass two sequences to the model and get a prediction of whether the two sentences are paraphrases or not. We need to handle the **two sequences as a pair**, and apply the appropriate preprocessing. Fortunately, the tokenizer can also take a pair of sequences and prepare it the way our BERT model expects.\n",
        "\n",
        "**token_type_ids** tells the model which part of the input is the first sentence and which is the second sentence."
      ],
      "metadata": {
        "id": "AwdiIEK7QEKN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHEFClS6MGtS",
        "outputId": "309c9688-e18b-4588-f42c-e6e8b9dc6381"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{ \n",
              "  'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 1012, 102, 2023, 2003, 1996, 2117, 2028, 1012, 102],\n",
              "  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
              "  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
              "}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer(\"This is the first sentence.\", \"This is the second one.\")\n",
        "inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we decode the IDs inside input_ids back to words:"
      ],
      "metadata": {
        "id": "qmsAPytIQbXq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mkb0JAtBMGtT",
        "outputId": "eafb8b0d-4e68-4b72-c0c3-66ca9d8af7b7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we see the model expects the inputs to be of the form [CLS] sentence1 [SEP] sentence2 [SEP] when there are two sentences. Aligning this with the token_type_ids gives us:\n",
        "\n",
        "['[CLS]', 'this', 'is', 'the', 'first', 'sentence', '.', '[SEP]', 'this', 'is', 'the', 'second', 'one', '.', '[SEP]'] <br>\n",
        "[      0,      0,    0,     0,       0,          0,   0,       0,      1,    1,     1,        1,     1,   1,       1]\n",
        "\n",
        "As you can see, the parts of the input corresponding to [CLS] sentence1 [SEP] all have a token type ID of 0, while the other parts, corresponding to sentence2 [SEP], all have a token type ID of 1."
      ],
      "metadata": {
        "id": "Bdm2qcX0QiIl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have seen how our tokenizer can deal with one pair of sentences, we can use it to tokenize our **whole dataset:** we can feed the tokenizer a list of pairs of sentences by giving it the list of first sentences, then the list of second sentences.\n",
        "\n",
        "This is also compatible with the padding and truncation options we saw in Chapter 2. So, one way to preprocess the training dataset is:"
      ],
      "metadata": {
        "id": "136SR49YRPpF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8l60LDokMGtT"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = tokenizer(\n",
        "    raw_datasets[\"train\"][\"sentence1\"],\n",
        "    raw_datasets[\"train\"][\"sentence2\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This works well, but it has the **disadvantage** of returning a dictionary (with our keys, input_ids, attention_mask, and token_type_ids, and values that are lists of lists).\n",
        "\n",
        "It will also only work if you have **enough RAM** to store your whole dataset during the tokenization.\n",
        "\n",
        "To keep the data as a dataset, we will use the **Dataset.map()** method.\n",
        "\n",
        "This also allows us some extra *flexibility*, if we need more preprocessing done than just tokenization.\n",
        "\n",
        "The **map()** method works by applying a function on each element of the dataset, so letâ€™s define a function that tokenizes our inputs:"
      ],
      "metadata": {
        "id": "QKAoTVOORxJX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAh6cGR0MGtT"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes a dictionary (like the items of our dataset) and returns a new dictionary with the keys input_ids, attention_mask, and token_type_ids.\n",
        "\n",
        "We can use the **batched=True** in our call to map(), which will greatly *speed up the tokenization.*\n",
        "\n",
        "Note that weâ€™ve left the padding argument out in our tokenization function for now. This is because **padding all the samples to the maximum length is not efficient**: itâ€™s better to pad the samples when weâ€™re building a **batch**, as then we only need to *pad to the maximum length in that batch*. This can **save a lot of time and processing power** when the inputs have very variable lengths!"
      ],
      "metadata": {
        "id": "JfJaqIw0SFCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is how we apply the **tokenization function on all our datasets** at once. Weâ€™re using **batched=True** in our call to map so the function is applied to multiple elements of our dataset at once, and not on each element separately. This allows for faster preprocessing.\n",
        "\n",
        "The way the ðŸ¤— Datasets library applies this processing is by adding new fields to the datasets, one for each key in the dictionary returned by the preprocessing function:"
      ],
      "metadata": {
        "id": "Extc8IRfTjVa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZV5a5q1MGtT",
        "outputId": "59c42894-71fd-4464-9a7b-c33c7bf2bbe8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
              "        num_rows: 3668\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
              "        num_rows: 408\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],\n",
              "        num_rows: 1725\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can even use **multiprocessing** when applying your preprocessing function with map() by passing along a **num_proc** argument.\n",
        "\n",
        "Our **tokenize_function** returns a **dictionary** with the keys input_ids, attention_mask, and token_type_ids, so those three fields are added to all splits of our dataset.\n",
        "\n",
        "*Note that we could also have changed existing fields if our preprocessing function returned a new value for an existing key in the dataset to which we applied map().*\n",
        "\n",
        "The last thing we will need to do is **pad** all the examples to the length of the longest element when we batch elements together â€” a technique we refer to as **dynamic padding**."
      ],
      "metadata": {
        "id": "w3FzirdRUAgK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Padding\n",
        "\n",
        "The function that is responsible for *putting together samples inside a batch* is called a **collate function**.\n",
        "\n",
        "- Itâ€™s an argument you can pass when you build a DataLoader,\n",
        "- the default being a function that will just convert your samples to PyTorch tensors and concatenate them.\n",
        "\n",
        "**This wonâ€™t be possible in our case since the inputs we have wonâ€™t all be of the same size.**\n",
        "\n",
        "To do this in practice, we have to **define a collate function** that *will apply the correct amount of padding to the items of the dataset *we want to batch together.\n",
        "\n",
        "ðŸ¤— Transformers library provides us with such a function via **DataCollatorWithPadding**. It takes a tokenizer when you instantiate it (to know which padding token to use, and whether the model expects padding to be on the left or on the right of the inputs) and will do everything you need:"
      ],
      "metadata": {
        "id": "rQPvdcqgUsLT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0rOosJ9MGtT"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test it letâ€™s grab a few samples from our training set that we would like to batch together.\n",
        "\n",
        "Here, we remove the columns idx, sentence1, and sentence2 as they wonâ€™t be needed and contain strings (and we canâ€™t create tensors with strings) and have a look at the lengths of each entry in the batch:"
      ],
      "metadata": {
        "id": "gylIOjcOVkz3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcpETjY5MGtU",
        "outputId": "47ce180d-2fe6-4158-9d28-a17ec85de610"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[50, 59, 47, 67, 59, 50, 62, 32]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "samples = tokenized_datasets[\"train\"][:8]\n",
        "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
        "[len(x) for x in samples[\"input_ids\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "No surprise, we get samples of varying length, from 32 to 67.\n",
        "\n",
        "Dynamic padding means the samples in this batch should all be padded to a length of 67, the maximum length inside the batch.\n",
        "\n",
        "Without dynamic padding, all of the samples would have to be padded to the maximum length in the **whole dataset,** or the maximum length the model can accept. Letâ€™s double-check that our data_collator is dynamically padding the batch properly:"
      ],
      "metadata": {
        "id": "cfylTrJmVzFE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PPHlhxaMGtU",
        "outputId": "a6e29165-ed01-42f5-bcbe-5d5ef78a7fd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'attention_mask': torch.Size([8, 67]),\n",
              " 'input_ids': torch.Size([8, 67]),\n",
              " 'token_type_ids': torch.Size([8, 67]),\n",
              " 'labels': torch.Size([8])}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch = data_collator(samples)\n",
        "{k: v.shape for k, v in batch.items()}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Processing the data (PyTorch)",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}