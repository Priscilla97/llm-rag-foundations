{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priscilla97/llm-rag-foundations/blob/main/01_NLP_basics/4_Tokenizers_(PyTorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1s5Liu01BYr"
      },
      "source": [
        "# Tokenizers (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3NmxNwq1BYz"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsaj6ZEZ1BY0"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizers are one of the core components of the NLP pipeline.\n",
        "\n",
        "They serve one purpose: to **translate text into data that can be processed by the model**. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data.\n",
        "\n",
        "In NLP tasks, the data that is generally processed is raw text. Here’s an example of such text:\n",
        "\n",
        "TEXT:\n",
        "Jim Henson was a puppeteer\n",
        "\n",
        "The goal is to find the **most meaningful representation** — that is, the one that makes the most *sense to the model* — and *the smallest representation.*\n",
        "\n",
        "EXAMPLES: <br>\n",
        "\n",
        "**Word-based**: split the raw text into words and find a numerical representation for each of them <br>\n",
        "*['Jim', 'Henson', 'was', 'a', 'puppeteer']*\n",
        "\n",
        "If we want to completely cover a language we’ll need to have an identifier for each word in the language, take over 500,000 words in the English language!\n",
        "\n",
        "Words like “dog” are represented differently from words like “dogs”, and the model will initially have no way of knowing that “dog” and “dogs” are similar.\n",
        "\n",
        "We also need a custom token to represent words that are **not in our vocabulary**.\n",
        "This is known as the “unknown” token, often represented as ”[UNK]” or ”<unk>”. It’s generally a bad sign if you see that the tokenizer is producing a lot of these tokens."
      ],
      "metadata": {
        "id": "_ICd-PpQ2DwL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to reduce the amount of unknown tokens is to go one level deeper, using a **character-based tokenizer**."
      ],
      "metadata": {
        "id": "kLzIF8Ul4L0P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SdttgVB1BY4",
        "outputId": "893022ab-91ce-4791-8835-cf9ea9b95051"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Jim', 'Henson', 'was', 'a', 'puppeteer']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
        "print(tokenized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Character-based\n",
        "Character-based tokenizers split the text into characters, rather than words. This has two primary benefits:\n",
        "\n",
        "- The vocabulary is much smaller.\n",
        "- There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters.\n",
        "\n",
        "This approach isn’t perfect either. Since the representation is now based on characters rather than words, it’s less meaningful: each character doesn’t mean a lot on its own.\n",
        "\n",
        "To get the best of both worlds, we can use a third technique that combines the two approaches: **subword tokenization**."
      ],
      "metadata": {
        "id": "aqOOHXzl4NBS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Subword tokenization\n",
        "**Subword tokenization** algorithms rely on the principle that **frequently used words should not be split into smaller subwords**, but rare words should be decomposed into meaningful subwords.\n",
        "\n",
        "EXAMPLE: <br>\n",
        "**“annoyingly”**: a rare word and could be decomposed into\n",
        "- “annoying”\n",
        "- “ly”.\n",
        "\n",
        "These are both likely to appear more frequently as standalone subwords.\n",
        "\n",
        "Here is an example showing how a subword tokenization algorithm would tokenize the sequence “**Let’s do tokenization**!“:\n",
        "- Let's < /w>\n",
        "- do < /w>\n",
        "- token\n",
        "- ization < /w>\n",
        "- ! < /w>\n",
        "\n",
        "“tokenization” was split into “token” and “ization”, two tokens that have a semantic meaning while being space-efficient. This allows us to have relatively good coverage with small vocabularies, and close to no unknown tokens.\n",
        "\n",
        "Unsurprisingly, there are **many more techniques** out there. To name a few:\n",
        "\n",
        "- Byte-level BPE, as used in GPT-2\n",
        "- WordPiece, as used in BERT\n",
        "- SentencePiece or Unigram, as used in several multilingual models"
      ],
      "metadata": {
        "id": "WUH9WP875ZiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and saving\n",
        "\n",
        "Loading and saving tokenizers is as simple as it is with models. Actually, it’s based on the same two methods: **from_pretrained()** and **save_pretrained()**. These methods will **load or save **the algorithm used by the tokenizer as well as its vocabulary (a bit like the weights of the model).\n",
        "\n",
        "Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the BertTokenizer class:"
      ],
      "metadata": {
        "id": "6kCQoHdc8JIK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvZVemy81BY7"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to AutoModel, the AutoTokenizer class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:"
      ],
      "metadata": {
        "id": "yvHpmP0k8s03"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2AAc01F1BY9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use tokenizer:"
      ],
      "metadata": {
        "id": "KitN-HAn8wjO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt88B_bY1BY9",
        "outputId": "15089a8a-9b14-4b70-f4d0-3d49e1b77394"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],\n",
              " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer(\"Using a Transformer network is simple\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOHdXX4t1BY_"
      },
      "outputs": [],
      "source": [
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(\"directory_on_my_computer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "The tokenization process is done by the tokenize() method of the tokenizer:"
      ],
      "metadata": {
        "id": "3LCESsv0DhUi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvQE6qpZ1BZA",
        "outputId": "a9b927a8-a6b0-4cd8-df25-f1cafc365c1f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "sequence = \"Using a Transformer network is simple\"\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tokenizer is a subword tokenizer: it splits the words until it obtains tokens that can be represented by its vocabulary. That’s the case here with transformer, which is split into two tokens: transform and ##er.\n",
        "\n",
        "# From tokens to input IDs\n",
        "The conversion to input IDs is handled by the convert_tokens_to_ids() tokenizer method:"
      ],
      "metadata": {
        "id": "4DPYPbAIDtJP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFPcqyGs1BZC",
        "outputId": "0bc1e1de-51a4-458b-ddc3-08909bf1f62b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[7993, 170, 11303, 1200, 2443, 1110, 3014]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "print(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoding\n",
        "Decoding is going the other way around: from vocabulary indices, we want to get a string. This can be done with the decode() method as follows:"
      ],
      "metadata": {
        "id": "mjNnoki2EBMY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvHqgHDY1BZD",
        "outputId": "0f896cb9-5f2e-4ea8-e607-d633fbdfce07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Using a Transformer network is simple'"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
        "print(decoded_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the decode method not only converts the indices back to tokens, but also **groups together the tokens that were part of the same words to produce a readable sentence**.\n",
        "\n",
        "This behavior will be extremely useful when we use models that predict new text (either text generated from a prompt, or for sequence-to-sequence problems like translation or summarization)."
      ],
      "metadata": {
        "id": "H9A7iooQEKB2"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Tokenizers (PyTorch)",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}