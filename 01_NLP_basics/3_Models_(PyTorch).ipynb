{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priscilla97/llm-rag-foundations/blob/main/01_NLP_basics/3_Models_(PyTorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s1UVkDAA3MK"
      },
      "source": [
        "# Models (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we’ll take a closer look at **creating and using models**. We’ll use the *AutoModel* class, which is handy when you want to instantiate any model from a checkpoint."
      ],
      "metadata": {
        "id": "yVAheQWYBG0E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoDGX7ulA3MS"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmbVAA15A3MV"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a Transformer\n",
        "Let’s begin by examining what happens when we instantiate an AutoModel:"
      ],
      "metadata": {
        "id": "EMzCmopMBODA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "model = AutoModel.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "Zj1w5m5uBlS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **checkpoint name** corresponds to:\n",
        "- a specific model architecture\n",
        "- weights\n",
        "\n",
        "In this case a **BERT model** with a basic architecture:\n",
        "- 12 layers,\n",
        "- 768 hidden size,\n",
        "- 12 attention heads)\n",
        "and cased inputs (uppercase/lowercase distinction is important).\n",
        "\n",
        "The **AutoModel** class and its associates are actually simple wrappers designed to fetch the appropriate model architecture for a given checkpoint. *It’s an “auto” class meaning* it will guess the appropriate model architecture for you and instantiate the correct model class.\n",
        "\n",
        "However, if you know the type of model you want to use, you can use the class that defines its architecture directly:"
      ],
      "metadata": {
        "id": "lSu8MV53Bsm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "model = BertModel.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "TRxdd69oCLGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and saving\n",
        "\n",
        "The models actually have the same *save_pretrained()* method, which saves the model’s weights and architecture configuration:"
      ],
      "metadata": {
        "id": "GvltF-5dCNuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"directory_on_my_computer\")"
      ],
      "metadata": {
        "id": "mc90IxoJCiUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will save two files to your disk:\n",
        "\n",
        "ls directory_on_my_computer\n",
        "config.json model.safetensors\n",
        "\n",
        "- the **config.json file** contains all the necessary *attributes* needed to build the model architecture. This file also contains some *metadata*, such as where the *checkpoint* originated.\n",
        "\n",
        "- the **pytorch_model.safetensors** file is known as the state dictionary; it contains all your *model’s* *weights*.\n",
        "\n",
        "The two files work together: the **configuration file** is needed to know about the model architecture, while the **model weights** are the parameters of the model.\n",
        "\n",
        "To reuse a saved model, use the from_pretrained() method again:"
      ],
      "metadata": {
        "id": "juC_JJlqCmA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "model = AutoModel.from_pretrained(\"directory_on_my_computer\")"
      ],
      "metadata": {
        "id": "-VBP0DEHDV1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you’re using a notebook, you can easily log in with this:"
      ],
      "metadata": {
        "id": "Y6BIVwEeDdpr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "3fpN2Q_uDezb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then you can push the model to the Hub with the push_to_hub() method:"
      ],
      "metadata": {
        "id": "fCWuIU_fDlH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"my-awesome-model\")"
      ],
      "metadata": {
        "id": "L875LGqYDnHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will upload the model files to the Hub, in a repository under your namespace named my-awesome-model. Then, anyone can load your model with the from_pretrained() method!"
      ],
      "metadata": {
        "id": "o7V3ZPR6z1R3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "model = AutoModel.from_pretrained(\"your-username/my-awesome-model\")"
      ],
      "metadata": {
        "id": "bBNqC5yjDwZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding text\n",
        "Tokenizers split the text into tokens and then **convert** these tokens into numbers.\n",
        "\n",
        "We can see this conversion through a simple tokenizer:"
      ],
      "metadata": {
        "id": "o1uBFmxAun_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "encoded_input = tokenizer(\"Hello, I'm a single sentence!\")\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "id": "ZXxeywYru_BH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT: <BR>\n",
        "{'input_ids': [101, 8667, 117, 1000, 1045, 1005, 1049, 2235, 17662, 12172, 1012, 102], <br>\n",
        " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], <br>\n",
        " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
        "\n",
        "We get a **dictionary** with the following fields:\n",
        "\n",
        "- **input_ids**: numerical representations of your tokens\n",
        "- **token_type_ids**: these tell the model which part of the input is sentence A and which is sentence B\n",
        "- **attention_mask**: this indicates which tokens should be attended to and which should not (discussed more in a bit)\n",
        "\n",
        "We can decode the input IDs to get back the original text:"
      ],
      "metadata": {
        "id": "BSNKFIMmvCkI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(encoded_input[\"input_ids\"])"
      ],
      "metadata": {
        "id": "thkkKgQRvVfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT <br>\n",
        "\"[CLS] Hello, I'm a single sentence! [SEP]\"\n",
        "\n",
        "You’ll notice that the tokenizer has added special tokens — **[CLS] and [SEP]** — required by the model. Not all models need special tokens; they’re utilized when a model was pretrained with them, in which case the tokenizer needs to add them as that model expects these tokens.\n",
        "\n",
        "You can encode **multiple sentences** at once, either by batching them together  or by passing a list.\n",
        "\n",
        "Note that when passing multiple sentences, the tokenizer returns a **list** for each sentence for each dictionary value."
      ],
      "metadata": {
        "id": "HaV68oLtvWM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(\"How are you?\", \"I'm fine, thank you!\", return_tensors=\"pt\")\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "id": "WvQr9ve8wAfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT: <br>\n",
        "\n",
        "\n",
        "{'input_ids': tensor([[  101,  1731,  1132,  1128,   136,   102], <br>\n",
        "         [  101,  1045,  1005,  1049,  2503,   117,  5763,  1128,   136,   102]]), <br>\n",
        " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), <br>\n",
        " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1],\n",
        "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
        "\n",
        "But there’s a **problem**: the two lists don’t have the same length! <br>**Arrays and tensors need to be rectangular**, so we can’t simply convert these lists to a PyTorch tensor (or NumPy array). The tokenizer provides an option for that: **padding**.\n",
        "        "
      ],
      "metadata": {
        "id": "OaqCQFk4wDO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding\n",
        "If we ask the tokenizer to pad the inputs, it will make all sentences the same length by adding a special padding token to the sentences that are shorter than the longest one:\n",
        "- padding = true"
      ],
      "metadata": {
        "id": "9Co_YZa-wtU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(\n",
        "    [\"How are you?\", \"I'm fine, thank you!\"], padding=True, return_tensors=\"pt\"\n",
        ")\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "id": "RyeKju9nw0zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT: <br>\n",
        "{'input_ids': tensor([[  101,  1731,  1132,  1128,   136,   102,     0,     0,     0,     0], <br>\n",
        "         [  101,  1045,  1005,  1049,  2503,   117,  5763,  1128,   136,   102]]), <br>\n",
        " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), <br>\n",
        " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
        "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
        "\n",
        "         "
      ],
      "metadata": {
        "id": "gNFmMDNMw1nR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have rectangular tensors! Note that the padding tokens have been encoded into input IDs with **ID 0**, and they have an **attention mask value of 0** as well. This is because those padding **tokens shouldn’t be analyzed by the model**: they’re not part of the actual sentence."
      ],
      "metadata": {
        "id": "ZKdIrjPAxDjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Truncating inputs\n",
        "The tensors might get **too big** to be processed by the model.\n",
        "\n",
        "For instance, BERT was only pretrained with sequences up to 512 tokens, so *it cannot process longer sequences*. If you have sequences longer than the model can handle, you’ll need to **truncate** them with the truncation parameter:"
      ],
      "metadata": {
        "id": "vKbdpBBxxVNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(\n",
        "    \"This is a very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very very long sentence.\",\n",
        "    truncation=True,\n",
        ")\n",
        "print(encoded_input[\"input_ids\"])"
      ],
      "metadata": {
        "id": "KuGGr5R9xg8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT: <br>\n",
        "[101, 1188, 1110, 170, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1505, 1179, 5650, 119, 102]\n",
        "\n",
        "By combining the padding and truncation arguments, you can make sure your tensors have the exact size you need:\n",
        "- truncation=True"
      ],
      "metadata": {
        "id": "ItFQ6XeSxjJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(\n",
        "    [\"How are you?\", \"I'm fine, thank you!\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=5,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "print(encoded_input)"
      ],
      "metadata": {
        "id": "jXX3ctlvxooG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT: <br>\n",
        "{'input_ids': tensor([[  101,  1731,  1132,  1128,   102],\n",
        "         [  101,  1045,  1005,  1049,   102]]), <br>\n",
        " 'token_type_ids': tensor([[0, 0, 0, 0, 0],\n",
        "         [0, 0, 0, 0, 0]]), <br>\n",
        " 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
        "         [1, 1, 1, 1, 1]])}"
      ],
      "metadata": {
        "id": "kM3trXnzxwGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding special tokens\n",
        "Special tokens are added to better represent the sentence boundaries, such as the **beginning** of a sentence **([CLS])** or **separator** between sentences **([SEP])**. Let’s look at a simple example:"
      ],
      "metadata": {
        "id": "nA21pPLpyEdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_input = tokenizer(\"How are you?\")\n",
        "print(encoded_input[\"input_ids\"])\n",
        "tokenizer.decode(encoded_input[\"input_ids\"])"
      ],
      "metadata": {
        "id": "GIOUscZfyTvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT: <br>\n",
        "[101, 1731, 1132, 1128, 136, 102] <br>\n",
        "'[CLS] How are you? [SEP]'\n",
        "\n",
        "These special tokens are automatically added by the tokenizer. Not all models need special tokens; they are primarily used when a model was pretrained with them, in which case the tokenizer will add them since the model expects them."
      ],
      "metadata": {
        "id": "a5x6qYpvyUWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Riassunto"
      ],
      "metadata": {
        "id": "W5vqIv4GyyF9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNNhh3hXA3MY"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "# Building the config\n",
        "config = BertConfig()\n",
        "\n",
        "# Building the model from the config\n",
        "model = BertModel(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OgFhE4uaA3Mb",
        "outputId": "d5bd6c1e-9c9b-4ce6-f72f-57e619556ff9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  [...]\n",
              "  \"hidden_size\": 768,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  [...]\n",
              "}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9fOQG_5A3Me"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "config = BertConfig()\n",
        "model = BertModel(config)\n",
        "\n",
        "# Model is randomly initialized!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cls5UvH9A3Mf"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "model = BertModel.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHM949ZvA3Mh"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"directory_on_my_computer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDayjtfMA3Mj"
      },
      "outputs": [],
      "source": [
        "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFZNqt6wA3Ml"
      },
      "outputs": [],
      "source": [
        "encoded_sequences = [\n",
        "    [101, 7592, 999, 102],\n",
        "    [101, 4658, 1012, 102],\n",
        "    [101, 3835, 999, 102],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAkj22vzA3Ml"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "model_inputs = torch.tensor(encoded_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making use of the tensors with the **model**. <br>\n",
        "While the model accepts a lot of different arguments, only the input IDs are necessary."
      ],
      "metadata": {
        "id": "BzFZBcHky8kN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XdMfyxnWA3Mm"
      },
      "outputs": [],
      "source": [
        "output = model(model_inputs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Models (PyTorch)",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}