{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priscilla97/llm-rag-foundations/blob/main/01_NLP_basics/2_Behind_the_pipeline_(PyTorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO65hbv0zCXr"
      },
      "source": [
        "# Behind the pipeline (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjJbZG8TzCXt"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bloLPz76zCXt"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eva8dt4K3jZx"
      },
      "source": [
        "Let‚Äôs start with a complete example, taking a look at what happened behind the scenes when we executed the following code in 1_pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m08bgNmdzCXu",
        "outputId": "eae55b04-5e42-45d5-b1df-975677202759"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n",
              " {'label': 'NEGATIVE', 'score': 0.9994558095932007}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkwpCia93jZy"
      },
      "source": [
        "This pipeline groups together three steps: preprocessing, passing the inputs through the model, and postprocessing.\n",
        "\n",
        "![](images/pipeline.png \"Pipeline process\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing with a tokenizer\n",
        "Like other neural networks, Transformer models can‚Äôt process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of.  <br>To do this we use a **tokenizer**, which will be responsible for:\n",
        "\n",
        "\n",
        "\n",
        "- Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n",
        "\n",
        "- Mapping each token to an integer\n",
        "\n",
        "- Adding additional inputs that may be useful to the model\n",
        "\n",
        "To do this, we use the **AutoTokenizer** class and *its from_pretrained()* method.\n",
        "\n",
        "Using the checkpoint name of our model, it will automatically fetch the data associated with the model‚Äôs tokenizer and cache it (so it‚Äôs only downloaded the first time you run the code below).\n",
        "\n",
        "Since the default checkpoint of the sentiment-analysis pipeline is *distilbert-base-uncased-finetuned-sst-2-english* (you can see its model card here), we run the following:"
      ],
      "metadata": {
        "id": "UtFhii7l3qxb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATYMnJvqzCXv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use Transformers without having to worry about which ML framework is used as a backend (PyTorch or Flax).\n",
        "\n",
        "Once we have the **tokenizer**, we can directly pass our sentences to it and we‚Äôll get back a dictionary that‚Äôs ready to feed to our model! The only thing left to do is to convert the list of input IDs to **tensors**."
      ],
      "metadata": {
        "id": "AM1nTZeq5Ik7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, Transformer models only accept tensors as input.\n",
        "\n",
        "You can think of them as NumPy arrays instead. A NumPy array can be a scalar (0D), a vector (1D), a matrix (2D), or have more dimensions. It‚Äôs effectively a tensor; other ML frameworks‚Äô tensors behave similarly, and are usually as simple to instantiate as NumPy arrays.\n",
        "\n",
        "The main things to remember here are that you can pass one sentence or a list of sentences, as well as specifying the type of tensors you want to get back (if no type is passed, you will get a list of lists as a result).\n",
        "\n",
        "To specify the type of tensors we want to get back (PyTorch or plain NumPy), we use the **return_tensors** argument:"
      ],
      "metadata": {
        "id": "yryY9Az9579a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLPFqDs1zCXv",
        "outputId": "8f5811fa-dad0-4eda-cca7-0693036f023a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\n",
              "    'input_ids': tensor([\n",
              "        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],\n",
              "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]\n",
              "    ]), \n",
              "    'attention_mask': tensor([\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
              "    ])\n",
              "}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **output** itself is a **dictionary** containing two keys:\n",
        "- input_ids\n",
        "- attention_mask.\n",
        "\n",
        "**input_ids** contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence.\n",
        "\n",
        "We‚Äôll explain what the attention_mask is later in this chapter."
      ],
      "metadata": {
        "id": "kzd0YmE16tzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Going through the model\n",
        "We can download our pretrained model the same way we did with our tokenizer.\n",
        "\n",
        "Transformers provides an AutoModel class which also has a from_pretrained() method.\n",
        "\n",
        "In this code snippet, we have downloaded the same checkpoint we used in our pipeline before (it should actually have been cached already) and instantiated a model with it.\n",
        "\n",
        "This architecture contains only the base Transformer module: given some inputs, it outputs what we‚Äôll call *hidden states*, also known as **features**. For each model input, we‚Äôll retrieve a high-dimensional vector representing the **contextual understanding of that input** by the Transformer model.\n",
        "\n",
        "While these hidden states can be useful on their own, they‚Äôre usually inputs to another part of the model, known as the *head*."
      ],
      "metadata": {
        "id": "SuivZbrB9VeG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_prDQGnzCXv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A high-dimensional vector?\n",
        "The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
        "\n",
        "- **Batch size**: The number of sequences processed at a time (2 in our example).\n",
        "\n",
        "- **Sequence length**: The length of the numerical representation of the sequence (16 in our example).\n",
        "\n",
        "- **Hidden size**: The vector dimension of each model input.\n",
        "\n",
        "It is said to be ‚Äúhigh dimensional‚Äù because of the last value. The **hidden size can be very large** (768 is common for smaller models, and in larger models this can reach 3072 or more).\n",
        "\n",
        "We can see this if we feed the inputs we preprocessed to our model.\n",
        "\n",
        "Note that the outputs of Transformers models behave like dictionaries.\n",
        "You can access the elements by attributes (like we did) or by key (*outputs[\"last_hidden_state\"]*), or even by index if you know exactly where the thing you are looking for is (*outputs[0]*)."
      ],
      "metadata": {
        "id": "u5Iedamf-LXg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms_3ZVhPzCXw",
        "outputId": "aaddfad3-5ef5-4ffc-9db8-855452e33b81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 16, 768])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model heads: Making sense out of numbers\n",
        "The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. They are usually composed of one or a few linear layers:\n",
        "\n",
        "![](https://github.com/Priscilla97/llm-rag-foundations/blob/main/01_NLP_basics/images/layers.png?raw=1)\n",
        "\n",
        "The output of the Transformer model is sent directly to the model head to be processed.\n",
        "\n",
        "In this diagram, the model is represented by its **embeddings layer** and the **subsequent layers**.\n",
        "\n",
        "The **embeddings layer** converts each input ID in the tokenized input into a vector that represents the associated token.\n",
        "\n",
        "The **subsequent layers** manipulate those vectors using the attention mechanism to produce the final representation of the sentences.\n",
        "\n",
        "There are many different architectures available in Transformers, with each one designed around tackling a specific task. Here is a non-exhaustive list:\n",
        "\n",
        "- *Model (retrieve the hidden states)\n",
        "- *ForCausalLM\n",
        "- *ForMaskedLM\n",
        "- *ForMultipleChoice\n",
        "- *ForQuestionAnswering\n",
        "- *ForSequenceClassification\n",
        "- *ForTokenClassification\n",
        "\n",
        "For our example, we will need a model with a sequence classification head (to be able to classify the sentences as positive or negative). So we use AutoModelForSequenceClassification:"
      ],
      "metadata": {
        "id": "F9hWJYcZ_MxP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0AebkXpzCXw"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now if we look at the shape of our outputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label).\n",
        "\n",
        "Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2."
      ],
      "metadata": {
        "id": "kFYLEi2SCaQq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_YcxrC-zCXw",
        "outputId": "5b420a41-d8c4-4485-d165-2bbec0f49363"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 2])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(outputs.logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Postprocessing the output\n"
      ],
      "metadata": {
        "id": "ykVdkRZGCkT3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-xeY1DMzCXw",
        "outputId": "94ed9c54-5d22-4e83-9ee8-c8a8d14cc60d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.5607,  1.6123],\n",
              "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model predicted [-1.5607, 1.6123] for the first sentence and [ 4.1692, -3.3464] for the second one.\n",
        "\n",
        "Those are not probabilities but **logits**, the raw, unnormalized scores outputted by the last layer of the model.\n",
        "\n",
        "To be converted to probabilities, they need to go through a **SoftMax** layer (all ü§ó Transformers models output the logits, as the loss function for training will generally fuse the last activation function, such as SoftMax, with the actual loss function, such as cross entropy):"
      ],
      "metadata": {
        "id": "rGyiuWJvCylZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_GD7c6SzCXx",
        "outputId": "d9955149-0b83-4a94-b09b-a2ffaa67cf83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[4.0195e-02, 9.5980e-01],\n",
              "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can see that the model predicted [0.0402, 0.9598] for the first sentence and [0.9995, 0.0005] for the second one. These are recognizable probability scores.\n",
        "\n",
        "To get the **labels** corresponding to each position, we can inspect the *id2label* attribute of the model config (more on this in the next section):"
      ],
      "metadata": {
        "id": "HQSO3jMnC__I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAYh5NLvzCXx",
        "outputId": "3506993b-2d39-4c65-cd57-3efee036afae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'NEGATIVE', 1: 'POSITIVE'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config.id2label"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can conclude that the model predicted the following:\n",
        "\n",
        "First sentence: NEGATIVE: 0.0402, POSITIVE: 0.9598\n",
        "\n",
        "Second sentence: NEGATIVE: 0.9995, POSITIVE: 0.0005"
      ],
      "metadata": {
        "id": "-Cehf13YDM-x"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Behind the pipeline (PyTorch)",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}