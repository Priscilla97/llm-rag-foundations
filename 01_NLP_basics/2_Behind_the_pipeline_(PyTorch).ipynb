{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priscilla97/llm-rag-foundations/blob/main/01_NLP_basics/2_Behind_the_pipeline_(PyTorch).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO65hbv0zCXr"
      },
      "source": [
        "# Behind the pipeline (PyTorch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjJbZG8TzCXt"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bloLPz76zCXt"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eva8dt4K3jZx"
      },
      "source": [
        "Let’s start with a complete example, taking a look at what happened behind the scenes when we executed the following code in 1_pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m08bgNmdzCXu",
        "outputId": "eae55b04-5e42-45d5-b1df-975677202759"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n",
              " {'label': 'NEGATIVE', 'score': 0.9994558095932007}]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkwpCia93jZy"
      },
      "source": [
        "This pipeline groups together three steps: preprocessing, passing the inputs through the model, and postprocessing.\n",
        "\n",
        "![](images/pipeline.png \"Pipeline process\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing with a tokenizer\n",
        "Like other neural networks, Transformer models can’t process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of.  <br>To do this we use a **tokenizer**, which will be responsible for:\n",
        "\n",
        "\n",
        "\n",
        "- Splitting the input into words, subwords, or symbols (like punctuation) that are called tokens\n",
        "\n",
        "- Mapping each token to an integer\n",
        "\n",
        "- Adding additional inputs that may be useful to the model\n",
        "\n",
        "To do this, we use the **AutoTokenizer** class and *its from_pretrained()* method.\n",
        "\n",
        "Using the checkpoint name of our model, it will automatically fetch the data associated with the model’s tokenizer and cache it (so it’s only downloaded the first time you run the code below).\n",
        "\n",
        "Since the default checkpoint of the sentiment-analysis pipeline is *distilbert-base-uncased-finetuned-sst-2-english* (you can see its model card here), we run the following:"
      ],
      "metadata": {
        "id": "UtFhii7l3qxb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATYMnJvqzCXv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use Transformers without having to worry about which ML framework is used as a backend (PyTorch or Flax).\n",
        "\n",
        "Once we have the **tokenizer**, we can directly pass our sentences to it and we’ll get back a dictionary that’s ready to feed to our model! The only thing left to do is to convert the list of input IDs to **tensors**."
      ],
      "metadata": {
        "id": "AM1nTZeq5Ik7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, Transformer models only accept tensors as input.\n",
        "\n",
        "You can think of them as NumPy arrays instead. A NumPy array can be a scalar (0D), a vector (1D), a matrix (2D), or have more dimensions. It’s effectively a tensor; other ML frameworks’ tensors behave similarly, and are usually as simple to instantiate as NumPy arrays.\n",
        "\n",
        "The main things to remember here are that you can pass one sentence or a list of sentences, as well as specifying the type of tensors you want to get back (if no type is passed, you will get a list of lists as a result).\n",
        "\n",
        "To specify the type of tensors we want to get back (PyTorch or plain NumPy), we use the **return_tensors** argument:"
      ],
      "metadata": {
        "id": "yryY9Az9579a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLPFqDs1zCXv",
        "outputId": "8f5811fa-dad0-4eda-cca7-0693036f023a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{\n",
              "    'input_ids': tensor([\n",
              "        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],\n",
              "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]\n",
              "    ]), \n",
              "    'attention_mask': tensor([\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
              "    ])\n",
              "}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_inputs = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **output** itself is a **dictionary** containing two keys:\n",
        "- input_ids\n",
        "- attention_mask.\n",
        "\n",
        "**input_ids** contains two rows of integers (one for each sentence) that are the unique identifiers of the tokens in each sentence.\n",
        "\n",
        "We’ll explain what the attention_mask is later in this chapter."
      ],
      "metadata": {
        "id": "kzd0YmE16tzA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_prDQGnzCXv"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms_3ZVhPzCXw",
        "outputId": "aaddfad3-5ef5-4ffc-9db8-855452e33b81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 16, 768])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs = model(**inputs)\n",
        "print(outputs.last_hidden_state.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0AebkXpzCXw"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_YcxrC-zCXw",
        "outputId": "5b420a41-d8c4-4485-d165-2bbec0f49363"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 2])"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(outputs.logits.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-xeY1DMzCXw",
        "outputId": "94ed9c54-5d22-4e83-9ee8-c8a8d14cc60d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-1.5607,  1.6123],\n",
              "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(outputs.logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_GD7c6SzCXx",
        "outputId": "d9955149-0b83-4a94-b09b-a2ffaa67cf83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[4.0195e-02, 9.5980e-01],\n",
              "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAYh5NLvzCXx",
        "outputId": "3506993b-2d39-4c65-cd57-3efee036afae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'NEGATIVE', 1: 'POSITIVE'}"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config.id2label"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Behind the pipeline (PyTorch)",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}